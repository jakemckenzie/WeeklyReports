\documentclass[english,12pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{siunitx}

\usepackage[hmargin=1in,vmargin=1in]{geometry}


\begin{document}

\begin{center}

\thispagestyle{empty}

$ $

\vspace{250pt}

\begin{bfseries}

{\Large Object Recognition and Path Smoothing Robot, Phase 2}

{\Huge Test Plan}

%{\Large $\langle$application and version to be tested$\rangle$}%

\end{bfseries}

\vspace{180pt}

University of Washington Tacoma, School of Engineering and Technology

%TIE-21204 Ohjelmistojen testaus%

\vspace{12pt}

Authors: 

Ammon Dodson \href{mailto:ammon0@uw.edu}{ammon0@uw.edu} 

Alex Marlow \href{mailto:alexmarlow117@gmail.com}{alexmarlow117@gmail.com} 

Jake McKenzie \href{mailto:jake314@uw.edu}{jake314@uw.edu}

Distribution: Matthew Tolentino

Document state: draft

Modified: \today

\end{center}

\newpage


\tableofcontents

\newpage


\section{Revision History}

\begin{itemize}
	\item v0.1: In this version of the testplan we defined the  
    scope of the problem with the introduction, test items and 
    approach.
    \item v0.2: In this version of the testplan we defined the  
    test items, schedule and approach.
\end{itemize}


\section{Introduction}
\subsection{Purpose}
This test plan describes the testing approach and overall 
framework that will drive the testing of the ORPS-Robot 
version 0.1 â€“ The Object Recognition and Path Smoothing 
robot. The document introduces:
\begin{itemize}
	\item[] Test Strategy: These are the rules the test will e based on including 
    the givens of the project (e.g.: start / end dates, objectives, assumptions); 
    description of the process to set up a valid test (e.g.: entry / exit criteria, 
    creation of test cases, specific tasks to perform and scheduling)
	\item[] Execution Strategy: describes how the test will be performed 
    and process to identify and report problems, and to fix and implement 
    fixes.
    \item[] Test Management: process to handle the logistics of the test 
    and all the events that come up during execution (e.g.: communications, 
    escalation procedures and risk mitigation)
\end{itemize}
\subsection{Project Overview}
The ORPS-Robot will be a platform for validating the research of Michael McCourt 
and a scheme for exploring object recognition via OpenCV with Robot Operating System, 
which is a powerful framework for writing robot software. There will be a demonstration 
of Simultaneous Localization and Mapping (SLAM). Together this will demostrate a ``finder
robot'' with applications in search and rescue and threat detection. Additionally there 
will be beacon triangulation and/or GPS to fuse additional location information into the 
SLAM or finder functionality.
\subsection{Audience}
Collaborative robotics software development for research in control systems 
for path smoothing. Collaboration in academic research is usually thought to mean 
equal partnership between two academic faculty members who are pursuing mutually 
interesting and beneficial research. In our case we are creating a platform which 
will serve Dr. McCourt research where deep understanding of the control system in 
place is not require on our part and a deep understanding of the robot are not 
require on the part of Dr. McCourt.\\\\
Creating truly, robust, general-purpose robot software is hard. As 
undergraduates using robot operating system framework allows us to encompass 
solving robotics problems in real-world variations in complex tasks and 
environments that no single individual, laboratory, or institution could 
hole to create completely on their own from scratch. The audience for this 
device is us, as it serves our education.\\\\
Applications in path smoothing and object recognition used in 
ORPS-Robot project are heavily used in semi-autonomous and autonomous 
vehicles. The knowledge gain in applying these skills in the ROS ecosystem 
should serve us to gaining a greater knowledge of the systems and 
practices of both control systems and object recognition.
\section{Test Items}
\subsection{Hardware Test Items}
These will have more descriptions in later revisions.
\begin{itemize}
    \item[] \ang{360} LiDAR for SLAM \& Navigation
    \item[] Rasberry Pi 3 Model B
    \item[] 32-bit ARM Cortex-M7 OpenCR
    \item[] DYNAMIXEL wheels
    \item[] Li-Po Battery 11.1V 1,800mAh 
    \item[] Xbox 360 Controller 
    \item[] Marvelmind Sensors
\end{itemize}
\subsection{Software Test Items}
At this point in the process we do not knwo what the overall architecture of the software will 
look like but we can have a general plan of attack for writing software tests using rostest. According 
to Paul Ammann's Introduciton to Software Testing, there are five levels of software testing: \\
\begin{itemize}
    \item[Level 0] There is no difference between testing and debugging.
    \item[Level 1] The purpose of testing is to show correctness.
    \item[Level 2] The purpose of testing is to show that software does not work.
    \item[Level 3] The purpose of testing is not to prove anything specific, 
    but to reduce the risk of using the software.
    \item[Level 4] Testing is a mental discipline that helps all IT professionals 
    develop higher-quality software.
\end{itemize}
For our purposes levels 0 through 2 will be used heavily in this process while level 3 and 4 will 
only be used when needed. To accomplish the task in level 0 we will be making heavy use of first 
GDB (C debugger) and PDB (Python debugger) along with Valgrind. Levels 1 and 2 will be tackled, 
hopefully, by making heavy use of \href{http://wiki.ros.org/rostest}{\textit{rostest}} which is an 
extension of roslaunch that enables roslaunch files to be used as text fixtures. Due to complex 
behaivors involved in this project we need to write tests and move on from functionality. When we 
introduce new functionality to the to ORPS-robot it will need to pass the old tests we wrote for it. 
if everything is functioning properly there should be no need to re-write our old tests. This will 
likely only be done with software strict aspects of the project as hardware is subject to change as 
the implementation changes over time. These tests, at least of the framing of this writing, will be 
to test the linking of different scripts and files written in the overall project. 
\subsection{System Test Items}
For the \ang{360} LiDAR for SLAM \& Navigation we will need to consult three seperate 
resources for testing the LiDAR. The first will be a github wiki page entitled 
``\href{https://github.com/robopeak/rplidar_ros/wiki/How-to-use-rplidar}{How to use rplidar}'' 
which is just a walkthrough of how to get rplidar ros packarge working. While this just gives 
a short introduction to rplidar, a more up to date tutorial on quickly getting rplidar up and 
running can be found on Service Engineering Research Area website under 
``\href{https://blog.zhaw.ch/icclab/rplidar/}{From unboxing RPLIDAR to running in ROS in 10 
minutes flat}''. The first goal with LiDAR appears to be establishing the range of the device 
as this what determines the quality of the information used in navigation. Given more time 
there will also involve be static and dynamic tests based on scanning calibration, 
speed of the LiDAR, vehicle speed, laser position feedback. Further testing will involve the use 
of a tutorial on husarion.com entitled \\\\
``\href{https://husarion.com/tutorials/ros-tutorials/6-slam-navigation/}{SLAM navigation}''. 
According to the Bachelor's Thesis of Felix Feik from Technical University of Munich,
the most difficult factor in indoor mapping using LiDAR is that the laser sensor always has 
the position or your map building and pose estimation will fail. The LiDAR needs a consistent, 
relative motion to map a building correctly. Glass doors and other visible objects will not 
be accounted for by the LiDAR as the signal will not penetrate glass. Long story short, to use 
SLAM properly we will need to test take into account the limitations of the device and leverage 
the libraries provided by robot operating system to do so. \\\\

As for testing our controller that will make heavy use of the 
\href{http://wiki.ros.org/joy#Microsoft_Xbox_360_Wired_Controller_for_Linux}{joy} 
package in ROS which is a library for generic Linux joysticks. 

\section{Approach}

Our tests will be derived from the requirements and specifications of our project and will follow 
the following methodological approach:

\begin{itemize}
    \item[]Acceptance Testing: Assess system (software \& hardware) interaction with respect 
    to user needs. For example, making sure the controller is responsive or camera is functioning 
    adequately.
    \item[]System Testing: Assesses system interaction to architecture design and overall behaivor. 
    For example a series of rostests that test the linking of each submodule we write for the overall 
    project.
    \item[]Integration Testing: The continous conjoining of different software artifacts to 
    serve the overall project development. For example, a test that ensures two individual modules are 
    linked properly.
    \item[]Unit Testing: Assesses system interaction with respect to implementation of specific subsystems. 
    For example, a test that ensures the controller is functioning properly standalone through the joy 
    package.
\end{itemize}
While these levels are emphasised in terms of when they are applied, it is more important to distinguish 
the types of faults listed above. They are all a continious part of the design and build process. At 
the end of the day there is an entire spectrum of testing, and much of it is hard to distinguish from 
development itself. Most tests look like unit tests and are quite simple. Other tests are more subtle, 
complex and critical. For such tests we need to use the tools laid out in this document to check our 
implemetation. In the testing process the standard behaivoral model should be employed, where 
execution of a program or task is respented by a behaivor. A behaivor is sequence of states while a state 
is an assignment of values of variables. Our program should be tested and modeled by a set of behaivors 
we define, representing possible executions. \\\\
Safety when testing can be specified using two things:\\
\begin{itemize}
    \item[-] The set of possible initial states.
    \item[-] A next-state relation, describing all possible successor states of any state. 
\end{itemize}
The above is a mindset that integrates the testing process into the development process.
\section{Fail Criteria}

\section{Testing Deliverables}

\section{Roles}

\section{Schedule}

\section{Testing Risks and Mitigation}

\section{Approvals}

\end{document}
